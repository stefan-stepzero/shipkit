---
name: shipkit-qa-visual
description: "Visual QA with dual artifacts: maintains a user-confirmed UI goals document (pages × personas × goals), then generates and runs Playwright tests against those goals. Auto-installs Chromium, runs headlessly, captures screenshots."
argument-hint: "<url-or-command> [--update-goals] [--run-only] [--screenshots]"
allowed-tools:
  - Read
  - Write
  - Edit
  - Glob
  - Grep
  - Bash
  - Task
---

# shipkit-qa-visual

Visual QA driven by explicit human intent — define which pages matter, which personas use them, and what goals each page serves, then let Claude generate and run Playwright tests that prove those goals are met.

---

## Dual Artifacts

This skill produces and maintains two distinct artifacts with different lifecycles:

### Artifact 1: `.shipkit/ui-goals.json` — Source of Truth (human-driven, persistent)

The goals document is the persistent record of what the UI is supposed to do. It is:
- **Created once** by scanning the codebase and proposing pages/personas/goals to the user
- **Confirmed by the user** before any tests are generated — this is the human-in-the-loop step
- **Accumulated across sessions** — subsequent runs ask "anything to add or change?" rather than overwriting
- **The authoritative input** for all test generation

This file captures decisions Claude cannot infer: which pages matter, which personas matter, and what each persona's goals are on each page.

### Artifact 2: `tests/e2e/*.spec.ts` — Derived Tests (regenerated from goals)

The test files are generated FROM ui-goals.json and are always considered derived:
- One spec file per page (`tests/e2e/{page-name}.spec.ts`)
- One `describe` block per persona
- One `test` per goal, with a comment tracing back to the goal
- **Regenerated** when goals change; tests that were not generated by this skill (custom tests) are preserved

---

## Arguments

Parse `$ARGUMENTS` before starting:

- **URL provided** (e.g. `/shipkit-qa-visual https://myapp.com`) → use as `baseUrl` for this run; update `ui-goals.json` if different from stored value
- **`--update-goals`** → run Steps 0–1 only; skip test generation and execution; useful for keeping goals current without a full run
- **`--run-only`** → skip Steps 1 and 3; run existing test files as-is; still loads `ui-goals.json` for the results-to-goals mapping in Step 5; useful for regression without regenerating tests
- **`--screenshots`** → capture screenshots on every assertion, not just failures
- **Empty** → proceed with the full flow (Steps 0 through 5)

Multiple flags can be combined: `/shipkit-qa-visual http://localhost:3000 --screenshots`

---

## Process

### Step 0: Setup

Check whether Playwright is available and create the config if needed.

**Check Playwright:**
```bash
npx playwright --version 2>/dev/null
```

If the command fails or returns no output, install Chromium only (not all browsers — this keeps the download to ~200MB instead of 600MB+):

```bash
npx -y playwright install chromium
```

Warn the user on first install: "Installing Playwright Chromium (~200MB). This may take a minute."

**Create config if missing:**

Check whether `tests/e2e/playwright.config.ts` exists. If not, create it using the template in the [Playwright Config Template](#playwright-config-template) section below. Do not overwrite an existing config.

---

### Step 1: Load or Create UI Goals

**If `.shipkit/ui-goals.json` exists:**

1. Read the file
2. Show a compact summary to the user:
   ```
   Loaded ui-goals.json
   Base URL: http://localhost:3000
   Pages: 4 | Personas: 2 | Goals: 11
   Last confirmed: 2026-02-15 | Last tested: 2026-02-18
   ```
3. Ask: "Anything to add or change before running? (Reply 'no' to proceed)"
4. If user provides updates, apply them and update `lastConfirmed` to the current ISO timestamp. Write the updated file.
5. If user replies "no" or similar, proceed with existing goals.

**If `.shipkit/ui-goals.json` does NOT exist:**

This is a first run. The user must confirm goals before tests are generated.

1. **Detect app URL** — use `$ARGUMENTS` URL if provided, otherwise check `baseUrl` is undefined and proceed; URL detection happens in Step 2
2. **Scan the codebase for routes/pages:**
   - Next.js: glob `app/**/page.{tsx,ts,jsx,js}` and `pages/**/*.{tsx,ts,jsx,js}` (excluding `_app`, `_document`, `api/`)
   - React Router: grep for `<Route` or `createBrowserRouter` patterns in `src/`
   - Express/other: grep for `router.get(` or `app.get(` patterns
3. **Infer personas from auth setup:**
   - Grep for auth patterns: `getSession`, `useSession`, `requireAuth`, `isAuthenticated`, `role`, `admin`
   - Propose personas based on what you find (e.g. Guest, Authenticated User, Admin)
   - If no auth found, default to a single "User" persona
4. **Propose goals per page per persona** — for each detected page, propose 1–3 observable goals per persona (things a user would actually verify on that page, not implementation details)
5. **Present to user for confirmation:**
   ```
   Proposed UI Goals

   Pages detected: 5
   Personas proposed: Guest, Logged-in User

   Please review and confirm — or reply with changes before I write this file.
   [show structured summary of pages × personas × goals]

   Reply 'confirm' to write ui-goals.json, or describe any changes.
   ```
6. **Wait for user confirmation.** Do NOT write ui-goals.json until the user confirms.
7. Write the confirmed goals to `.shipkit/ui-goals.json` using the schema in the [ui-goals.json Schema](#ui-goals-json-schema) section below.

---

### Step 2: Detect App URL

Determine the `baseUrl` for test execution:

1. If `ui-goals.json` has a `baseUrl` set → use it
2. If a URL was passed in `$ARGUMENTS` → use it (update `ui-goals.json` if different)
3. Probe common dev server ports:
   ```bash
   curl -s -o /dev/null -w "%{http_code}" http://localhost:3000 2>/dev/null
   curl -s -o /dev/null -w "%{http_code}" http://localhost:5173 2>/dev/null
   curl -s -o /dev/null -w "%{http_code}" http://localhost:8080 2>/dev/null
   ```
   Use the first port that returns a non-error response.
4. If still unresolved, check `package.json` for a `dev` script to infer the port, then ask the user: "What URL is your app running at?"

If the user is testing a deployed URL, accept it directly. Update `ui-goals.json` `baseUrl` field with whatever URL is resolved.

---

### Step 3: Generate Tests

Generate Playwright test files from `ui-goals.json`. Skip this step if `--run-only` was passed.

**For each page in `ui-goals.json`:**

Create (or overwrite) `tests/e2e/{page-name}.spec.ts` where `page-name` is derived from the page's `name` field in kebab-case.

**Test file structure:**

```typescript
// Generated by shipkit-qa-visual — do not edit manually
// Goals source: .shipkit/ui-goals.json
// Page: {page.name} ({page.path})

import { test, expect } from '@playwright/test';

test.describe('{persona.name}', () => {
  // Goal ID: {goal.id} — {goal.goal}
  test('{goal.goal}', async ({ page }) => {
    await page.goto('{baseUrl}{page.path}');
    // Assert goal is met
    // ...
  });
});
```

**Selector strategy (in priority order):**
1. Use `selectors` from the goal in `ui-goals.json` when specified
2. Fall back to `data-testid` attributes: `page.getByTestId('...')`
3. Fall back to ARIA roles: `page.getByRole('button', { name: '...' })`
4. Fall back to visible text: `page.getByText('...')`

**Preserve custom tests:** Before overwriting a spec file, check whether it contains the `// Generated by shipkit-qa-visual` header. If the header is absent, the file is custom — do not overwrite it. Log a warning: "Skipping {file} — custom test file detected."

**Add screenshot captures:**

At key assertions (or all assertions if `--screenshots` was passed):
```typescript
await page.screenshot({ path: `tests/e2e/screenshots/{page-name}-{persona-id}.png` });
```

---

### Step 4: Run Tests

Execute the generated tests headlessly:

```bash
npx playwright test --project=chromium --reporter=list tests/e2e/
```

**On failure:**

1. Capture a screenshot of the failing state (if not already captured)
2. Read the error output carefully — distinguish between:
   - **Selector not found** → goal's selector may be stale; attempt to update selector in ui-goals.json and regenerate
   - **Navigation error** → baseUrl or page path may be wrong; surface to user
   - **Assertion failure** → the UI does not meet the stated goal; report to user, do not auto-fix
3. Attempt one fix for selector-type failures, then re-run
4. If still failing after one fix attempt, report the failure with the screenshot path rather than looping

Screenshots go to `tests/e2e/screenshots/`. Create the directory if it does not exist:
```bash
mkdir -p tests/e2e/screenshots
```

---

### Step 5: Report

After tests complete, produce a results table mapped back to goals (not just test names):

```
## Visual QA Report

Base URL: http://localhost:3000
Run: 2026-02-20T14:32:00Z
Tests: 11 total | 9 passed | 2 failed

| Page        | Persona        | Goal                            | Status | Screenshot                          |
|-------------|----------------|---------------------------------|--------|-------------------------------------|
| Homepage    | Guest          | Can see hero section and CTA    | PASS   |                                     |
| Checkout    | Logged-in User | Can complete purchase flow      | FAIL   | screenshots/checkout-logged-in.png  |
```

Update `lastTestedAt` in `ui-goals.json` to the current ISO timestamp.

If all tests pass: "All goals verified. Visual QA clean."

If failures remain: List each failed goal with the screenshot path and a brief description of what was observed. Do not auto-fix assertion failures — these represent real UI regressions that require human decisions.

---

## ui-goals.json Schema

When creating or updating `.shipkit/ui-goals.json`, always conform to this structure:

```json
{
  "baseUrl": "http://localhost:3000",
  "personas": [
    {
      "id": "guest",
      "name": "Guest User",
      "description": "Not logged in, browsing publicly"
    }
  ],
  "pages": [
    {
      "path": "/",
      "name": "Homepage",
      "goals": [
        {
          "id": "homepage-guest-hero",
          "persona": "guest",
          "goal": "Can see hero section and CTA button",
          "priority": "high",
          "selectors": ["[data-testid='hero']", "[data-testid='cta-primary']"]
        }
      ]
    }
  ],
  "lastConfirmed": "2026-02-20T14:00:00Z",
  "lastTestedAt": "2026-02-20T14:32:00Z"
}
```

**Field notes:**
- `personas[].id` — used as the persona key in goal references; use lowercase kebab-case
- `pages[].goals[].id` — stable identifier for traceability; use `{page-name}-{persona-id}-{short-slug}` format; generated tests reference this ID in comments
- `pages[].goals[].persona` — must match a `personas[].id` value
- `pages[].goals[].priority` — one of `"high"`, `"medium"`, `"low"`; used for reporting order (high first) and can filter test generation in future iterations
- `pages[].goals[].selectors` — optional array of CSS or data-testid selectors to use in assertions; omit if you want Claude to infer selectors at generation time
- `lastConfirmed` — updated whenever the user reviews and approves the goals
- `lastTestedAt` — updated after each successful test run

---

## Playwright Config Template

Create `tests/e2e/playwright.config.ts` with this content when the file does not exist:

```typescript
import { defineConfig, devices } from '@playwright/test';

export default defineConfig({
  testDir: './',
  timeout: 30000,
  retries: 1,
  reporter: 'list',
  use: {
    headless: true,
    screenshot: 'only-on-failure',
    video: 'off',
    trace: 'off',
  },
  projects: [
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
    },
  ],
  outputDir: './screenshots',
});
```

This config lives at `tests/e2e/playwright.config.ts`. The `npx playwright test` command in Step 4 picks it up automatically because it's in the test directory.

---

## Integration

This skill is used by:

| Skill | How |
|-------|-----|
| `shipkit-verify` | References `shipkit-qa-visual --run-only` as an optional visual verification step in deeper reviews |

---

## Context Files This Skill Reads

| File | Purpose |
|------|---------|
| `.shipkit/ui-goals.json` | Persistent goals — loaded on every run |
| `package.json` | Dev server port inference, Playwright dependency check |
| `tests/e2e/*.spec.ts` | Existing tests — checked before regenerating |

---

## Context Files This Skill Writes

| File | When |
|------|------|
| `.shipkit/ui-goals.json` | Created on first run; updated when goals change or tests complete |
| `tests/e2e/{page-name}.spec.ts` | Generated from goals (one per page) |
| `tests/e2e/playwright.config.ts` | Created on first run if absent |
| `tests/e2e/screenshots/*.png` | Captured on failure or with `--screenshots` flag |

---

<!-- SECTION:after-completion -->
## After Completion

Visual QA results are now mapped to explicit human-confirmed goals. Next steps depend on what was found:

- **All goals passing** → consider running `/shipkit-preflight` before deployment
- **Goals failing** → review the failing goals and screenshots; decide whether to update the UI or update the goals
- **Goals stale** → run `/shipkit-qa-visual --update-goals` to refine the goals document without a full test run

The `ui-goals.json` file is the long-term artifact here — keep it current as the app evolves.
<!-- /SECTION:after-completion -->

<!-- SECTION:success-criteria -->
## Success Criteria

- [ ] Playwright availability checked; Chromium installed if missing
- [ ] `tests/e2e/playwright.config.ts` exists (created if absent)
- [ ] `.shipkit/ui-goals.json` exists and has been confirmed by the user this session or in a prior session
- [ ] Goals presented to user for confirmation on first run; not written until confirmed
- [ ] Subsequent runs ask about changes before proceeding
- [ ] Test files generated with one spec per page, describe per persona, test per goal
- [ ] Each test includes a `// Goal ID:` comment tracing to the goal ID
- [ ] Custom (non-generated) test files are preserved
- [ ] Tests run headlessly via `npx playwright test --project=chromium`
- [ ] Screenshots captured to `tests/e2e/screenshots/` on failure (or all assertions with `--screenshots`)
- [ ] Results table maps outcomes back to goals, not just test names
- [ ] `lastTestedAt` updated in `ui-goals.json` after test run
<!-- /SECTION:success-criteria -->
